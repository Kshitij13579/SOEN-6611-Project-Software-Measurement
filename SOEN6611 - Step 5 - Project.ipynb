{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ced60b",
   "metadata": {},
   "source": [
    "# SOEN 6611 - Step 5 - Implementation of Measurement Process\n",
    "\n",
    "### Business Goal\n",
    "Improve quality of big data for machine learning model.\n",
    "\n",
    "### Machine Learning Goal\n",
    "The goal of the machine learning model is to predict whether a particular individual is at high/low credit risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e3ef702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad38a47",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "Out dataset is a credit card dataset containing details of individuals like age, income etc. and a target value stating whether the particular individual is at high or low credit risk. The data is coleected in a form of CSV file and is loaded into the memory using the pandas library. \n",
    "\n",
    "#### Assumption\n",
    "We are considering this file as a single dataset split into three time frames T1, T2 and T3.<br>\n",
    "$Nds$ = number of datasets = 1\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/samuelcortinhas/credit-card-classification-clean-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdfe8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 4000\n",
      "Count: 3000\n",
      "Count: 2828\n"
     ]
    }
   ],
   "source": [
    "#import data into dataframe\n",
    "\n",
    "df = pd.read_csv('data_manipulated.csv')\n",
    "\n",
    "#split data frame into three parts assuming each part is a separate time frame\n",
    "\n",
    "df_t1 = df.iloc[:4000]\n",
    "df_t2 = df.iloc[4000:7000]\n",
    "df_t3 = df.iloc[7000:]\n",
    "\n",
    "print('Count:', df_t1.shape[0])\n",
    "print('Count:', df_t2.shape[0])\n",
    "print('Count:', df_t3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7617d53",
   "metadata": {},
   "source": [
    "### Global Variables\n",
    "\n",
    "num_request - number of requests to database. \n",
    "<br>\n",
    "num_successful_request - number of successful requests to database. <br>\n",
    "**It is assumed that dataframe is equivalent to database and each call to dataframe is considered as request. If the call does not return any error then num_successful_requests is incremented.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16e4a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_request = 0\n",
    "num_successful_request = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a0651",
   "metadata": {},
   "source": [
    "### Base Measure Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8352d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method : recCount(df_list) : method to calculate total number of records in multiple datasets <br>\n",
    "# Param : df_list : list of multiple dataframes \n",
    "# Return : Integer : returns total number of records in multiple datasets\n",
    "def recCount(df_list):\n",
    "    global num_request\n",
    "    global num_successful_request\n",
    "    num_records = 0\n",
    "    for i in df_list:\n",
    "        num_request += 1\n",
    "        num_records += i.shape[0]\n",
    "        num_successful_request += 1\n",
    "    \n",
    "    return num_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a50298b",
   "metadata": {},
   "source": [
    "## Big Data Quality Indicators\n",
    "### Veracity\n",
    "\n",
    "Veracity refers to the degree that data is accurate, trusted and precise. It is not only the accuracy of the data itself but the trustworthiness of the data source, type, and processing of it.\n",
    "<br>\n",
    "$Veracity = W_{acc} * Accuracy + W_{comp} * Completeness + W_{curr} * Currentness + W_{avail} * Availability$\n",
    "\n",
    "Derived Measures:\n",
    "- Accuracy\n",
    "- Completeness\n",
    "- Currentness\n",
    "- Availability\n",
    "\n",
    "#### Accuracy\n",
    "Degree to which data has attributes that correctly represent the true value of the intended attribute of a concept or event in a specific context of use.\n",
    "<br><br>\n",
    "$H_{acc} = log_2(Lbd) - (1/Lbd) * \\sum{p_j * log_2(p_j)}$ <br>\n",
    "$H_{max} = log_2(Lbd)$ <br>\n",
    "$Accuracy = H_{acc} / H_{max}$ <br>\n",
    "\n",
    "$Lbd$ : number of records in dataset <br>\n",
    "$p_j$ : total number of duplicate records in dataset <br>\n",
    "$H_{acc}$ : entropy of multiple datasets\n",
    "\n",
    "#### Completeness\n",
    "Degree to which subject data associated with an entity has values for all expected attributes and related entity instances in a specific context of use.\n",
    "<br><br>\n",
    "$Com_m(MDS) = \\frac{rec\\_no\\_null(MDS)}{Lbd(MDS)}$\n",
    "\n",
    "$Rec\\_no\\_null (MDS)$ : Frequency of records (in MDS) with no null values<br>\n",
    "$Lbd$ : number of records in dataset <br>\n",
    "\n",
    "#### Currentness\n",
    "Degree to which data has attributes that are of the right age in a specific context of use. \n",
    "<br><br>\n",
    "$Currentness(MDS) = \\frac{rec\\_acc\\_age(MDS)}{Lbd(MDS)}$\n",
    "\n",
    "$Rec\\_acc\\_age(MDS)$ : Provides the total number of records with ages that fall within the acceptable range based on the upper and lower quartiles of the Box and Whisker. <br>\n",
    "$Lbd(MDS)$ : Total Number of records in MDS\n",
    "\n",
    "#### Availability\n",
    "Degree to which data has attributes that enable it to be retrieved by authorized users and/or applications in a specific context of use. \n",
    "<br><br>\n",
    "$Availability(MDS) = \\frac{N\\_succ\\_req (MDS)}{N\\_req (MDS)} $\n",
    "\n",
    "$N\\_succ\\_req (MDS)$ : Number of successful requests (from an API, server, datastore, origins of data, etc) <br>\n",
    "$N\\_req (MDS)$ : Number of requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23ee7e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "def getAccuracy(df_list):\n",
    "    num_records = recCount(df_list)\n",
    "    global num_request\n",
    "    global num_successful_request\n",
    "    sum=0\n",
    "    for i in df_list:\n",
    "        num_request += 1\n",
    "        if i.duplicated().sum() != 0:\n",
    "            sum += (i.duplicated().sum() * np.log2(i.duplicated().sum()))\n",
    "        num_successful_request += 1\n",
    "    \n",
    "    \n",
    "    Hacc = np.log2(num_records) - (1/num_records) * sum\n",
    "    Hmax = np.log2(num_records)\n",
    "    \n",
    "    return (Hacc / Hmax) \n",
    "\n",
    "#Completeness\n",
    "def getCompleteness(df_list):\n",
    "    num_records = recCount(df_list)\n",
    "    global num_request\n",
    "    global num_successful_request\n",
    "    rec_null=0\n",
    "    for i in df_list:\n",
    "        num_request += 1\n",
    "        rec_null += i.isnull().any(axis=1).sum()\n",
    "        num_successful_request += 1\n",
    "    \n",
    "    return ((num_records - rec_null) / num_records)\n",
    "    \n",
    "\n",
    "#Currentness\n",
    "def getCurrentness(df_list):\n",
    "    num_records = recCount(df_list)\n",
    "    \n",
    "    #converting string date to pandas Date type\n",
    "    combine_df = pd.concat([df_t1,df_t2,df_t3])\n",
    "    combine_df = combine_df.dropna(subset=['Date'])\n",
    "    combine_df['Date'] = pd.to_datetime(combine_df['Date'], format='%m/%d/%Y')\n",
    "    combine_df = combine_df.sort_values(by='Date')\n",
    "    \n",
    "    #performing box and whisker analysis on Date column\n",
    "    n = combine_df.shape[0]\n",
    "    lower = np.ceil((n+1)/4).astype('int64')-1\n",
    "    higher = np.ceil((3/4) *(n+1)).astype('int64')-1\n",
    "    date_lower = combine_df.iloc[lower]['Date']\n",
    "    date_higher = combine_df.iloc[higher]['Date']\n",
    "    count = combine_df.loc[combine_df['Date'].between(date_lower,date_higher,inclusive='both')].shape[0]\n",
    "    \n",
    "    return count / num_records\n",
    "\n",
    "#Availability\n",
    "def getAvailability():\n",
    "    return num_successful_request / num_request\n",
    "\n",
    "#Veracity\n",
    "def getVeracity(df_list, w_acc=1/4, w_comp=1/4, w_curr=1/4 ,w_avail = 1/4):\n",
    "    \n",
    "    if((w_acc + w_comp + w_curr + w_avail) > 1):\n",
    "        print(\"Sum of weights is greater than one\")\n",
    "        return\n",
    "    \n",
    "    return (w_acc * getAccuracy(df_list)) + (w_comp * getCompleteness(df_list)) + (w_curr * getCurrentness(df_list)) + (w_avail * getAvailability())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f4e90",
   "metadata": {},
   "source": [
    "### Vincularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c6a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d05ca102",
   "metadata": {},
   "source": [
    "### Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e1e1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compliance\n",
    "def getCompliance(df_list):\n",
    "    # total number of datasets\n",
    "    nds_df_list = len(df_list)\n",
    "    sum_mds_comp = 0\n",
    "    \n",
    "    for i in df_list:\n",
    "        num_comp_rec = i['PCI_compliant'].value_counts()['Yes']\n",
    "        total_rec = i.shape[0]\n",
    "        i_comp = num_comp_rec/total_rec\n",
    "        sum_mds_comp += i_comp\n",
    "    \n",
    "    mds_compliance = sum_mds_comp/nds_df_list\n",
    "    print(\"Compliance: \", mds_compliance)\n",
    "    return mds_compliance\n",
    "\n",
    "# Credability\n",
    "def getCredability(df_list):\n",
    "    # total number of datasets\n",
    "    nds_df_list = len(df_list)\n",
    "    num_cred_dataset = 0\n",
    "    \n",
    "    for i in df_list:\n",
    "        num_cred_dataset += 1\n",
    "    \n",
    "    mds_credability = num_cred_dataset/num_cred_dataset\n",
    "    print(\"Credability: \", mds_credability)\n",
    "    return mds_credability\n",
    "\n",
    "# Validity\n",
    "def getValidity(credability, cred_weight, compliance, comp_weight):\n",
    "    ver = credability*cred_weight + compliance*comp_weight\n",
    "    return ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f68822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compliance:  0.92125\n",
      "Credability:  1.0\n",
      "Validity for Time frame 1:  0.9606250000000001\n"
     ]
    }
   ],
   "source": [
    "# Validity for Time frame 1\n",
    "print(\"Validity for Time frame 1: \", getValidity(getCompliance([df_t1]), 0.5, getCredability([df_t1]), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bfdc5b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compliance:  0.9163333333333333\n",
      "Credability:  1.0\n",
      "Validity for Time frame 2:  0.9581666666666666\n"
     ]
    }
   ],
   "source": [
    "# Validity for Time frame 2\n",
    "print(\"Validity for Time frame 2: \", getValidity(getCompliance([df_t2]), 0.5, getCredability([df_t2]), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af07f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compliance:  0.923974540311174\n",
      "Credability:  1.0\n",
      "Validity for Time frame 3:  0.9619872701555869\n"
     ]
    }
   ],
   "source": [
    "# Validity for Time frame 3\n",
    "print(\"Validity for Time frame 3: \", getValidity(getCompliance([df_t3]), 0.5, getCredability([df_t3]), 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd49c01",
   "metadata": {},
   "source": [
    "### Measuring Indicators before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aa02fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '10-11-2012' does not match format '%m/%d/%Y' (match)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\bigdata-lab\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:509\u001b[0m, in \u001b[0;36m_to_datetime_with_format\u001b[1;34m(arg, orig_arg, name, tz, fmt, exact, errors, infer_datetime_format)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 509\u001b[0m     values, tz \u001b[38;5;241m=\u001b[39m \u001b[43mconversion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m     dta \u001b[38;5;241m=\u001b[39m DatetimeArray(values, dtype\u001b[38;5;241m=\u001b[39mtz_to_dtype(tz))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bigdata-lab\\lib\\site-packages\\pandas\\_libs\\tslibs\\conversion.pyx:359\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.datetime_to_datetime64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unrecognized value type: <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m getAccuracy([df_t1,df_t2,df_t3])\n\u001b[0;32m      3\u001b[0m completeness \u001b[38;5;241m=\u001b[39m getCompleteness([df_t1,df_t2,df_t3])\n\u001b[1;32m----> 4\u001b[0m currentness \u001b[38;5;241m=\u001b[39m \u001b[43mgetCurrentness\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_t1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_t2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_t3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m availability \u001b[38;5;241m=\u001b[39m getAvailability()\n\u001b[0;32m      6\u001b[0m veracity \u001b[38;5;241m=\u001b[39m getVeracity([df_t1,df_t2,df_t3])\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mgetCurrentness\u001b[1;34m(df_list)\u001b[0m\n\u001b[0;32m     38\u001b[0m combine_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_t1,df_t2,df_t3])\n\u001b[0;32m     39\u001b[0m combine_df \u001b[38;5;241m=\u001b[39m combine_df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 40\u001b[0m combine_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombine_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm/\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m combine_df \u001b[38;5;241m=\u001b[39m combine_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#performing box and whisker analysis on Date column\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bigdata-lab\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:887\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m    885\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 887\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    888\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bigdata-lab\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:393\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 393\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_to_datetime_with_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_datetime_format\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bigdata-lab\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:513\u001b[0m, in \u001b[0;36m_to_datetime_with_format\u001b[1;34m(arg, orig_arg, name, tz, fmt, exact, errors, infer_datetime_format)\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatetimeIndex\u001b[38;5;241m.\u001b[39m_simple_new(dta, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bigdata-lab\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:500\u001b[0m, in \u001b[0;36m_to_datetime_with_format\u001b[1;34m(arg, orig_arg, name, tz, fmt, exact, errors, infer_datetime_format)\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m _box_as_indexlike(result, utc\u001b[38;5;241m=\u001b[39mutc, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;66;03m# fallback\u001b[39;00m\n\u001b[1;32m--> 500\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_datetime_format\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# Fallback to try to convert datetime objects if timezone-aware\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;66;03m#  datetime objects are found without passing `utc=True`\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bigdata-lab\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:436\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, tz, fmt, exact, errors, infer_datetime_format)\u001b[0m\n\u001b[0;32m    433\u001b[0m utc \u001b[38;5;241m=\u001b[39m tz \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 436\u001b[0m     result, timezones \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fmt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fmt:\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _return_parsed_timezone_results(result, timezones, tz, name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bigdata-lab\\lib\\site-packages\\pandas\\_libs\\tslibs\\strptime.pyx:150\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data '10-11-2012' does not match format '%m/%d/%Y' (match)"
     ]
    }
   ],
   "source": [
    "#Veracity\n",
    "accuracy = getAccuracy([df_t1,df_t2,df_t3])\n",
    "completeness = getCompleteness([df_t1,df_t2,df_t3])\n",
    "currentness = getCurrentness([df_t1,df_t2,df_t3])\n",
    "availability = getAvailability()\n",
    "veracity = getVeracity([df_t1,df_t2,df_t3])\n",
    "\n",
    "print('Accuracy:',accuracy)\n",
    "print('Completeness:',completeness)\n",
    "print('Currentness:',currentness)\n",
    "print('Availability:',availability)\n",
    "print('Veracity:',veracity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vincularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d045d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compliance\n",
    "def getCompliance(df_list):\n",
    "    # total number of datasets\n",
    "    nds_df_list = len(df_list)\n",
    "    sum_mds_comp = 0\n",
    "    \n",
    "    for i in df_list:\n",
    "        num_comp_rec = i['PCI_compliant'].value_counts()['Yes']\n",
    "        total_rec = i.shape[0]\n",
    "        i_comp = num_comp_rec/total_rec\n",
    "        sum_mds_comp += i_comp\n",
    "    \n",
    "    mds_compliance = sum_mds_comp/nds_df_list\n",
    "    print(\"Compliance: \", mds_compliance)\n",
    "    return mds_compliance\n",
    "\n",
    "# Credability\n",
    "def getCredability(df_list):\n",
    "    # total number of datasets\n",
    "    nds_df_list = len(df_list)\n",
    "    num_cred_dataset = 0\n",
    "    \n",
    "    for i in df_list:\n",
    "        num_cred_dataset += 1\n",
    "    \n",
    "    mds_credability = num_cred_dataset/num_cred_dataset\n",
    "    print(\"Credability: \", mds_credability)\n",
    "    return mds_credability\n",
    "\n",
    "# Validity\n",
    "def getValidity(credability, cred_weight, compliance, comp_weight):\n",
    "    ver = credability*cred_weight + compliance*comp_weight\n",
    "    return ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3625030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compliance:  0.92125\n",
      "Credability:  1.0\n",
      "Validity for Time frame 1:  0.9606250000000001\n",
      "Compliance:  0.9163333333333333\n",
      "Credability:  1.0\n",
      "Validity for Time frame 2:  0.9581666666666666\n",
      "Compliance:  0.923974540311174\n",
      "Credability:  1.0\n",
      "Validity for Time frame 3:  0.9619872701555869\n"
     ]
    }
   ],
   "source": [
    "# Validity for Time frame 1\n",
    "print(\"Validity for Time frame 1: \", getValidity(getCompliance([df_t1]), 0.5, getCredability([df_t1]), 0.5))\n",
    "\n",
    "# Validity for Time frame 2\n",
    "print(\"Validity for Time frame 2: \", getValidity(getCompliance([df_t2]), 0.5, getCredability([df_t2]), 0.5))\n",
    "\n",
    "# Validity for Time frame 3\n",
    "print(\"Validity for Time frame 3: \", getValidity(getCompliance([df_t3]), 0.5, getCredability([df_t3]), 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a9265a",
   "metadata": {},
   "source": [
    "### Analyzing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc60bfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                  0\n",
       "Gender             16\n",
       "Own_car            16\n",
       "Own_property       16\n",
       "Work_phone         44\n",
       "Phone              20\n",
       "Email              16\n",
       "Unemployed         16\n",
       "Num_children       16\n",
       "Num_family         44\n",
       "Account_length     16\n",
       "Total_income       16\n",
       "Age                16\n",
       "Years_employed     16\n",
       "Income_type        16\n",
       "Education_type     16\n",
       "Family_status      16\n",
       "Housing_type       16\n",
       "Occupation_type    16\n",
       "Target             16\n",
       "Date               16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5db374",
   "metadata": {},
   "source": [
    "There are null values in all the columns except ID and these rows with null values needs to be removed as part of cleaning the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe85bed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking duplicate records\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f95a82",
   "metadata": {},
   "source": [
    "There are 11 duplicate rows which needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0f6772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Numeric Values Gender: 0\n",
      "Non Numeric Values Car: 0\n",
      "Non Numeric Values Property: 0\n",
      "Non Numeric Values Work Phone: 0\n",
      "Non Numeric Values Phone: 32\n",
      "Non Numeric Values Email: 0\n",
      "Non Numeric Values Unemployed: 56\n",
      "Non Numeric Values Target: 52\n"
     ]
    }
   ],
   "source": [
    "# checking for gender, own_car, own_property, work_phone, phone, email, unemployed and Target columns not having values 1 or 0\n",
    "count_gender = (~df['Gender'].isna() & ~df['Gender'].isin([1,0])).sum()\n",
    "count_car = (~df['Own_car'].isna() & ~df['Own_car'].isin([1,0])).sum()\n",
    "count_property = (~df['Own_property'].isna() & ~df['Own_property'].isin([1,0])).sum()\n",
    "count_work_phone = (~df['Work_phone'].isna() & ~df['Work_phone'].isin([1,0])).sum()\n",
    "count_phone = (~df['Phone'].isna() & ~df['Phone'].isin([1,0])).sum()\n",
    "count_email = (~df['Email'].isna() & ~df['Email'].isin([1,0])).sum()\n",
    "count_unemployed = (~df['Unemployed'].isna() & ~df['Unemployed'].isin(['1','0'])).sum()\n",
    "count_target = (~df['Target'].isna() & ~df['Target'].isin([1,0])).sum()\n",
    "\n",
    "print('Non Numeric Values Gender:', count_gender)\n",
    "print('Non Numeric Values Car:', count_car)\n",
    "print('Non Numeric Values Property:', count_property)\n",
    "print('Non Numeric Values Work Phone:', count_work_phone)\n",
    "print('Non Numeric Values Phone:', count_phone)\n",
    "print('Non Numeric Values Email:', count_email)\n",
    "print('Non Numeric Values Unemployed:', count_unemployed)\n",
    "print('Non Numeric Values Target:', count_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672d1a8",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbab4efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 3966\n",
      "Count: 2966\n",
      "Count: 2828\n"
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    \n",
    "    #remove rows with atleast 1 null value\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Function call\n",
    "df_t1_processed = preprocess(df_t1)\n",
    "df_t2_processed = preprocess(df_t2)\n",
    "df_t3_processed = preprocess(df_t3)\n",
    "\n",
    "print('Count:', df_t1_processed.shape[0])\n",
    "print('Count:', df_t2_processed.shape[0])\n",
    "print('Count:', df_t3_processed.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332e1ec",
   "metadata": {},
   "source": [
    "### Measuring Indicators after processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35cbfe",
   "metadata": {},
   "source": [
    "### Visualizing and Interpreting Indicators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
