{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ced60b",
   "metadata": {},
   "source": [
    "# SOEN 6611 - Step 5 - Implementation of Measurement Process\n",
    "\n",
    "### Business Goal\n",
    "Improve quality of big data for machine learning model.\n",
    "\n",
    "### Machine Learning Goal\n",
    "The goal of the machine learning model is to predict whether a particular individual is at high/low credit risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6e3ef702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", 'This pattern has match groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad38a47",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "Out dataset is a credit card dataset containing details of individuals like age, income etc. and a target value stating whether the particular individual is at high or low credit risk. The data is coleected in a form of CSV file and is loaded into the memory using the pandas library. \n",
    "\n",
    "#### Assumption\n",
    "We are considering this file as a single dataset split into three time frames T1, T2 and T3.<br>\n",
    "$Nds$ = number of datasets = 1\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/samuelcortinhas/credit-card-classification-clean-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6cdfe8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 4000\n",
      "Count: 3000\n",
      "Count: 2920\n",
      "ID                   int64\n",
      "Gender             float64\n",
      "Own_car            float64\n",
      "Own_property       float64\n",
      "Work_phone          object\n",
      "Phone              float64\n",
      "Email              float64\n",
      "Unemployed          object\n",
      "Num_children       float64\n",
      "Num_family         float64\n",
      "Account_length     float64\n",
      "Total_income       float64\n",
      "Age                float64\n",
      "Years_employed     float64\n",
      "Income_type         object\n",
      "Education_type      object\n",
      "Family_status       object\n",
      "Housing_type        object\n",
      "Occupation_type     object\n",
      "Target             float64\n",
      "Date                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#import data into dataframe\n",
    "\n",
    "df = pd.read_csv('data_manipulated.csv')\n",
    "\n",
    "#split data frame into three parts assuming each part is a separate time frame\n",
    "\n",
    "df_t1 = df.iloc[:4000]\n",
    "df_t2 = df.iloc[4000:7000]\n",
    "df_t3 = df.iloc[7000:]\n",
    "\n",
    "print('Count:', df_t1.shape[0])\n",
    "print('Count:', df_t2.shape[0])\n",
    "print('Count:', df_t3.shape[0])\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7617d53",
   "metadata": {},
   "source": [
    "### Global Variables\n",
    "\n",
    "num_request - number of requests to database. \n",
    "<br>\n",
    "num_successful_request - number of successful requests to database. <br>\n",
    "**It is assumed that dataframe is equivalent to database and each call to dataframe is considered as request. If the call does not return any error then num_successful_requests is incremented.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16e4a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_request = 0\n",
    "num_successful_request = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a0651",
   "metadata": {},
   "source": [
    "### Base Measure Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8352d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method : recCount(df_list) : method to calculate total number of records in multiple datasets <br>\n",
    "# Param : df_list : list of multiple dataframes \n",
    "# Return : Integer : returns total number of records in multiple datasets\n",
    "def recCount(df_list):\n",
    "    global num_request\n",
    "    global num_successful_request\n",
    "    num_records = 0\n",
    "    for i in df_list:\n",
    "        num_request += 1\n",
    "        num_records += i.shape[0]\n",
    "        num_successful_request += 1\n",
    "    \n",
    "    return num_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a50298b",
   "metadata": {},
   "source": [
    "## Big Data Quality Indicators\n",
    "### Veracity\n",
    "\n",
    "Veracity refers to the degree that data is accurate, trusted and precise. It is not only the accuracy of the data itself but the trustworthiness of the data source, type, and processing of it.\n",
    "<br>\n",
    "$Veracity = W_{acc} * Accuracy + W_{comp} * Completeness + W_{curr} * Currentness + W_{avail} * Availability$\n",
    "\n",
    "Derived Measures:\n",
    "- Accuracy\n",
    "- Completeness\n",
    "- Currentness\n",
    "- Availability\n",
    "\n",
    "#### Accuracy\n",
    "Degree to which data has attributes that correctly represent the true value of the intended attribute of a concept or event in a specific context of use.\n",
    "<br><br>\n",
    "$H_{acc} = log_2(Lbd) - (1/Lbd) * \\sum{p_j * log_2(p_j)}$ <br>\n",
    "$H_{max} = log_2(Lbd)$ <br>\n",
    "$Accuracy = H_{acc} / H_{max}$ <br>\n",
    "\n",
    "$Lbd$ : number of records in dataset <br>\n",
    "$p_j$ : total number of duplicate records in dataset <br>\n",
    "$H_{acc}$ : entropy of multiple datasets\n",
    "\n",
    "#### Completeness\n",
    "Degree to which subject data associated with an entity has values for all expected attributes and related entity instances in a specific context of use.\n",
    "<br><br>\n",
    "$Com_m(MDS) = \\frac{rec\\_no\\_null(MDS)}{Lbd(MDS)}$\n",
    "\n",
    "$Rec\\_no\\_null (MDS)$ : Frequency of records (in MDS) with no null values<br>\n",
    "$Lbd$ : number of records in dataset <br>\n",
    "\n",
    "#### Currentness\n",
    "Degree to which data has attributes that are of the right age in a specific context of use. \n",
    "<br><br>\n",
    "$Currentness(MDS) = \\frac{rec\\_acc\\_age(MDS)}{Lbd(MDS)}$\n",
    "\n",
    "$Rec\\_acc\\_age(MDS)$ : Provides the total number of records with ages that fall within the acceptable range based on the upper and lower quartiles of the Box and Whisker. <br>\n",
    "$Lbd(MDS)$ : Total Number of records in MDS\n",
    "\n",
    "#### Availability\n",
    "Degree to which data has attributes that enable it to be retrieved by authorized users and/or applications in a specific context of use. \n",
    "<br><br>\n",
    "$Availability(MDS) = \\frac{N\\_succ\\_req (MDS)}{N\\_req (MDS)} $\n",
    "\n",
    "$N\\_succ\\_req (MDS)$ : Number of successful requests (from an API, server, datastore, origins of data, etc) <br>\n",
    "$N\\_req (MDS)$ : Number of requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23ee7e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "def getAccuracy(df_list):\n",
    "    num_records = recCount(df_list)\n",
    "    global num_request\n",
    "    global num_successful_request\n",
    "    sum=0\n",
    "    for i in df_list:\n",
    "        num_request += 1\n",
    "        if i.duplicated().sum() != 0:\n",
    "            sum += (i.duplicated().sum() * np.log2(i.duplicated().sum()))\n",
    "        num_successful_request += 1\n",
    "    \n",
    "    \n",
    "    Hacc = np.log2(num_records) - (1/num_records) * sum\n",
    "    Hmax = np.log2(num_records)\n",
    "    \n",
    "    return (Hacc / Hmax) \n",
    "\n",
    "#Completeness\n",
    "def getCompleteness(df_list):\n",
    "    num_records = recCount(df_list)\n",
    "    global num_request\n",
    "    global num_successful_request\n",
    "    rec_null=0\n",
    "    for i in df_list:\n",
    "        num_request += 1\n",
    "        rec_null += i.isnull().any(axis=1).sum()\n",
    "        num_successful_request += 1\n",
    "    \n",
    "    return ((num_records - rec_null) / num_records)\n",
    "    \n",
    "\n",
    "#Currentness\n",
    "def getCurrentness(df_list):\n",
    "    num_records = recCount(df_list)\n",
    "    \n",
    "    #converting string date to pandas Date type\n",
    "    combine_df = pd.concat([df_t1,df_t2,df_t3])\n",
    "    combine_df = combine_df.dropna(subset=['Date'])\n",
    "    combine_df['Date'] = pd.to_datetime(combine_df['Date'], format='%m/%d/%Y')\n",
    "    combine_df = combine_df.sort_values(by='Date')\n",
    "    \n",
    "    #performing box and whisker analysis on Date column\n",
    "    n = combine_df.shape[0]\n",
    "    lower = np.ceil((n+1)/4).astype('int64')-1\n",
    "    higher = np.ceil((3/4) *(n+1)).astype('int64')-1\n",
    "    date_lower = combine_df.iloc[lower]['Date']\n",
    "    date_higher = combine_df.iloc[higher]['Date']\n",
    "    count = combine_df.loc[combine_df['Date'].between(date_lower,date_higher,inclusive='both')].shape[0]\n",
    "    \n",
    "    return count / num_records\n",
    "\n",
    "#Availability\n",
    "def getAvailability():\n",
    "    return num_successful_request / num_request\n",
    "\n",
    "#Veracity\n",
    "def getVeracity(df_list, w_acc=1/4, w_comp=1/4, w_curr=1/4 ,w_avail = 1/4):\n",
    "    \n",
    "    if((w_acc + w_comp + w_curr + w_avail) > 1):\n",
    "        print(\"Sum of weights is greater than one\")\n",
    "        return\n",
    "    \n",
    "    return (w_acc * getAccuracy(df_list)) + (w_comp * getCompleteness(df_list)) + (w_curr * getCurrentness(df_list)) + (w_avail * getAvailability())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f4e90",
   "metadata": {},
   "source": [
    "### Vincularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c6a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d05ca102",
   "metadata": {},
   "source": [
    "### Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e1e1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate String date format\n",
    "# Using dateutil.parser.parse\n",
    "from dateutil import parser\n",
    "\n",
    "def isValidDate(test_str):\n",
    "    # initializing format\n",
    "    format = \"%d/%m/%Y\"\n",
    "    # checking if format matches the date\n",
    "    res = True\n",
    "    # using try-except to check for truth value\n",
    "    try:\n",
    "        res = bool(parser.parse(test_str))\n",
    "    except ValueError:\n",
    "        res = False\n",
    "    return res\n",
    "\n",
    "def isCompliant(row):\n",
    "    gender_comp =  row.Gender != '' and row.Gender == 0 or row.Gender == 1\n",
    "    car_comp = row.Own_car != '' and row.Own_car == 0 or row.Own_car == 1\n",
    "    property_comp = row.Own_property != '' and row.Own_property == 0 or row.Own_property == 1\n",
    "    work_phone_comp = row.Work_phone != '' and row.Work_phone == '0' or row.Work_phone == '1'\n",
    "    phone_comp = row.Phone != '' and row.Phone == 0 or row.Phone == 1\n",
    "    email_comp = row.Email != '' and row.Email == 0 or row.Email == 1 \n",
    "    unemployed_comp = row.Unemployed != '' and row.Unemployed == '0' or row.Unemployed == '1'\n",
    "    children_comp = row.Num_children != '' and row.Num_children >= 0\n",
    "    family_comp = row.Num_family != '' and row.Num_family >= 0\n",
    "    account_comp = row.Account_length != '' and row.Account_length >= 0\n",
    "    income_comp = row.Total_income != '' and row.Total_income >= 0\n",
    "    age_comp = row.Age != '' and row.Age >= 0\n",
    "    years_comp = row.Years_employed != '' and row.Years_employed >= 0\n",
    "    income_type_comp = isinstance(row.Income_type, str) and row.Income_type != '' and len(row.Income_type) >= 0\n",
    "    education_type_comp = isinstance(row.Education_type, str) and row.Education_type != '' and len(row.Education_type) >= 0\n",
    "    fam_status_comp = isinstance(row.Family_status, str) and row.Family_status != '' and len(row.Family_status) >= 0\n",
    "    housing_type_comp = isinstance(row.Housing_type, str) and row.Housing_type != '' and len(row.Housing_type) >= 0\n",
    "    occ_type_comp = isinstance(row.Occupation_type, str) and row.Occupation_type != '' and len(row.Occupation_type) >= 0\n",
    "    target_comp = row.Target != '' and row.Target == 0 or row.Target == 1 \n",
    "    date_comp = row.Date != '' and isValidDate(str(row.Date))\n",
    "    \n",
    "    if gender_comp and car_comp and property_comp and work_phone_comp and phone_comp and email_comp and unemployed_comp\\\n",
    "        and children_comp and family_comp and account_comp and income_comp and age_comp and years_comp and income_type_comp\\\n",
    "        and education_type_comp and fam_status_comp and housing_type_comp and occ_type_comp and target_comp and date_comp:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "# Compliance\n",
    "def getCompliance(df_list):\n",
    "    # total number of datasets\n",
    "    nds_df_list = len(df_list)\n",
    "    sum_mds_comp = 0\n",
    "    \n",
    "    for df in df_list:\n",
    "        num_comp_rec = 0\n",
    "        for row in df.itertuples(index=False):\n",
    "            if isCompliant(row):\n",
    "                num_comp_rec += 1\n",
    "        total_rec = df.shape[0]\n",
    "        df_comp = num_comp_rec/total_rec\n",
    "        sum_mds_comp += df_comp\n",
    "    \n",
    "    mds_compliance = sum_mds_comp/nds_df_list\n",
    "    print(\"Compliance: \", mds_compliance)\n",
    "    return mds_compliance\n",
    "\n",
    "# Credability\n",
    "def getCredability(df_list):\n",
    "    # total number of datasets\n",
    "    nds_df_list = len(df_list)\n",
    "    num_cred_dataset = 0\n",
    "    \n",
    "    for i in df_list:\n",
    "        num_cred_dataset += 1\n",
    "    \n",
    "    mds_credability = num_cred_dataset/num_cred_dataset\n",
    "    print(\"Credability: \", mds_credability)\n",
    "    return mds_credability\n",
    "\n",
    "# Validity\n",
    "def getValidity(credability, cred_weight, compliance, comp_weight):\n",
    "    ver = credability*cred_weight + compliance*comp_weight\n",
    "    return ver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd49c01",
   "metadata": {},
   "source": [
    "### Measuring Indicators before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4aa02fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9997110554880555\n",
      "Completeness: 0.9931451612903226\n",
      "Currentness: 0.49959677419354837\n",
      "Availability: 1.0\n",
      "Veracity: 0.8731132477429816\n"
     ]
    }
   ],
   "source": [
    "#Veracity\n",
    "accuracy = getAccuracy([df_t1,df_t2,df_t3])\n",
    "completeness = getCompleteness([df_t1,df_t2,df_t3])\n",
    "currentness = getCurrentness([df_t1,df_t2,df_t3])\n",
    "availability = getAvailability()\n",
    "veracity = getVeracity([df_t1,df_t2,df_t3])\n",
    "\n",
    "print('Accuracy:',accuracy)\n",
    "print('Completeness:',completeness)\n",
    "print('Currentness:',currentness)\n",
    "print('Availability:',availability)\n",
    "print('Veracity:',veracity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b3b8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vincularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d045d5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compliance:  0.991\n",
      "Credability:  1.0\n",
      "Validity for Time frame 1:  0.9955\n",
      "Compliance:  0.9895\n",
      "Credability:  1.0\n",
      "Validity for Time frame 2:  0.99475\n",
      "Compliance:  0.9824977168949772\n",
      "Credability:  1.0\n",
      "Validity for Time frame 3:  0.9912488584474886\n"
     ]
    }
   ],
   "source": [
    "#Validity\n",
    "# Validity for Time frame 1\n",
    "print(\"Validity for Time frame 1: \", getValidity(getCompliance([df_t1]), 0.5, getCredability([df_t1]), 0.5))\n",
    "\n",
    "# Validity for Time frame 2\n",
    "print(\"Validity for Time frame 2: \", getValidity(getCompliance([df_t1, df_t2]), 0.5, getCredability([df_t1, df_t2]), 0.5))\n",
    "\n",
    "# Validity for Time frame 3\n",
    "print(\"Validity for Time frame 3: \", getValidity(getCompliance([df_t1, df_t2, df_t3]), 0.5, getCredability([df_t1, df_t2, df_t3]), 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a9265a",
   "metadata": {},
   "source": [
    "### Analyzing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc60bfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                  0\n",
       "Gender             16\n",
       "Own_car            16\n",
       "Own_property       16\n",
       "Work_phone         44\n",
       "Phone              20\n",
       "Email              16\n",
       "Unemployed         16\n",
       "Num_children       16\n",
       "Num_family         44\n",
       "Account_length     16\n",
       "Total_income       16\n",
       "Age                16\n",
       "Years_employed     16\n",
       "Income_type        16\n",
       "Education_type     16\n",
       "Family_status      16\n",
       "Housing_type       16\n",
       "Occupation_type    16\n",
       "Target             16\n",
       "Date               16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5db374",
   "metadata": {},
   "source": [
    "There are null values in all the columns except ID and these rows with null values needs to be removed as part of cleaning the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe85bed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking duplicate records\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f95a82",
   "metadata": {},
   "source": [
    "There are 11 duplicate rows which needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0f6772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Numeric Values Gender: 8\n",
      "Non Numeric Values Car: 0\n",
      "Non Numeric Values Property: 0\n",
      "Non Numeric Values Work Phone: 13\n",
      "Non Numeric Values Phone: 124\n",
      "Non Numeric Values Email: 0\n",
      "Non Numeric Values Unemployed: 135\n",
      "Non Numeric Values Target: 144\n"
     ]
    }
   ],
   "source": [
    "# checking for gender, own_car, own_property, work_phone, phone, email, unemployed and Target columns not having values 1 or 0\n",
    "count_gender = (~df['Gender'].isna() & ~df['Gender'].isin([1,0])).sum()\n",
    "count_car = (~df['Own_car'].isna() & ~df['Own_car'].isin([1,0])).sum()\n",
    "count_property = (~df['Own_property'].isna() & ~df['Own_property'].isin([1,0])).sum()\n",
    "count_work_phone = (~df['Work_phone'].isna() & ~df['Work_phone'].isin(['1','0'])).sum()\n",
    "count_phone = (~df['Phone'].isna() & ~df['Phone'].isin([1,0])).sum()\n",
    "count_email = (~df['Email'].isna() & ~df['Email'].isin([1,0])).sum()\n",
    "count_unemployed = (~df['Unemployed'].isna() & ~df['Unemployed'].isin(['1','0'])).sum()\n",
    "count_target = (~df['Target'].isna() & ~df['Target'].isin([1,0])).sum()\n",
    "\n",
    "print('Non Numeric Values Gender:', count_gender)\n",
    "print('Non Numeric Values Car:', count_car)\n",
    "print('Non Numeric Values Property:', count_property)\n",
    "print('Non Numeric Values Work Phone:', count_work_phone)\n",
    "print('Non Numeric Values Phone:', count_phone)\n",
    "print('Non Numeric Values Email:', count_email)\n",
    "print('Non Numeric Values Unemployed:', count_unemployed)\n",
    "print('Non Numeric Values Target:', count_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672d1a8",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "24606e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfterNull: 9745\n",
      "Duplicates: 9745\n",
      "9745\n"
     ]
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "\n",
    "df = df[~df['Gender'].isna() & df['Gender'].isin([1,0])]\n",
    "df = df[~df['Own_car'].isna() & df['Own_car'].isin([1,0])]\n",
    "df = df[~df['Own_property'].isna() & df['Own_property'].isin([1,0])]\n",
    "df = df[~df['Work_phone'].isna() & df['Work_phone'].isin(['1','0'])]\n",
    "df = df[~df['Phone'].isna() & df['Phone'].isin([1,0])]\n",
    "df = df[~df['Email'].isna() & df['Email'].isin([1,0])]\n",
    "df = df[~df['Unemployed'].isna() & df['Unemployed'].isin(['1','0'])]\n",
    "df = df[~df['Target'].isna() & df['Target'].isin([1,0])]\n",
    "df = df[~df['Num_children'].isna() & df['Num_children'].astype(str).str.contains('[0-9]+')]\n",
    "df = df[~df['Num_family'].isna() & df['Num_family'].astype(str).str.contains('[0-9]+')]\n",
    "df = df[~df['Account_length'].isna() & df['Account_length'].astype(str).str.contains('[0-9]+')]\n",
    "df = df[~df['Total_income'].isna() & df['Total_income'].astype(str).str.contains('[0-9]+')]\n",
    "df = df[~df['Age'].isna() & df['Age'].astype(str).str.contains('[0-9]+')]\n",
    "df['Age'] = df['Age'].apply(np.floor).astype('int64')\n",
    "df = df[~df['Years_employed'].isna() & df['Years_employed'].astype(str).str.contains('[0-9]+')]\n",
    "df = df[~df['Income_type'].isna() & df['Income_type'].astype(str).str.contains('[a-zA-Z]+')]\n",
    "df = df[~df['Education_type'].isna() & df['Education_type'].astype(str).str.contains('[a-zA-Z]+')]\n",
    "df = df[~df['Family_status'].isna() & df['Family_status'].astype(str).str.contains('[a-zA-Z]+')]\n",
    "df = df[~df['Housing_type'].isna() & df['Housing_type'].astype(str).str.contains('[a-zA-Z]+')]\n",
    "df = df[~df['Occupation_type'].isna() & df['Occupation_type'].astype(str).str.contains('[a-zA-Z]+')]\n",
    "df = df[~df['Date'].isna() & df['Date'].astype(str).str.contains('^([1-9]|1[0-2])(\\/)([1-9]|1[0-9]|2[0-9]|3[0-1])(\\/)([12][0-9][0-9][0-9])$')]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fbab4efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 3953\n",
      "Count: 2964\n",
      "Count: 2828\n"
     ]
    }
   ],
   "source": [
    "def processData(df):\n",
    "    \n",
    "    #remove rows with atleast 1 null value\n",
    "    df=df.dropna()\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df = df[~df['Gender'].isna() & df['Gender'].isin([1,0])]\n",
    "    df = df[~df['Own_car'].isna() & df['Own_car'].isin([1,0])]\n",
    "    df = df[~df['Own_property'].isna() & df['Own_property'].isin([1,0])]\n",
    "    df = df[~df['Work_phone'].isna() & df['Work_phone'].isin(['1','0'])]\n",
    "    df = df[~df['Phone'].isna() & df['Phone'].isin([1,0])]\n",
    "    df = df[~df['Email'].isna() & df['Email'].isin([1,0])]\n",
    "    df = df[~df['Unemployed'].isna() & df['Unemployed'].isin(['1','0'])]\n",
    "    df = df[~df['Target'].isna() & df['Target'].isin([1,0])]\n",
    "    df = df[~df['Num_children'].isna() & df['Num_children'].astype(str).str.contains('[0-9]+')]\n",
    "    df = df[~df['Num_family'].isna() & df['Num_family'].astype(str).str.contains('[0-9]+')]\n",
    "    df = df[~df['Account_length'].isna() & df['Account_length'].astype(str).str.contains('[0-9]+')]\n",
    "    df = df[~df['Total_income'].isna() & df['Total_income'].astype(str).str.contains('[0-9]+')]\n",
    "    df = df[~df['Age'].isna() & df['Age'].astype(str).str.contains('[0-9]+')]\n",
    "    df['Age'] = df['Age'].apply(np.floor).astype('int64')\n",
    "    df = df[~df['Years_employed'].isna() & df['Years_employed'].astype(str).str.contains('[0-9]+')]\n",
    "    df = df[~df['Income_type'].isna() & df['Income_type'].astype(str).str.contains('[a-zA-Z]+')]\n",
    "    df = df[~df['Education_type'].isna() & df['Education_type'].astype(str).str.contains('[a-zA-Z]+')]\n",
    "    df = df[~df['Family_status'].isna() & df['Family_status'].astype(str).str.contains('[a-zA-Z]+')]\n",
    "    df = df[~df['Housing_type'].isna() & df['Housing_type'].astype(str).str.contains('[a-zA-Z]+')]\n",
    "    df = df[~df['Occupation_type'].isna() & df['Occupation_type'].astype(str).str.contains('[a-zA-Z]+')]\n",
    "    df = df[~df['Date'].isna() & df['Date'].astype(str).str.contains('^([1-9]|1[0-2])(\\/)([1-9]|1[0-9]|2[0-9]|3[0-1])(\\/)([12][0-9][0-9][0-9])$')]\n",
    "\n",
    "    return df\n",
    "\n",
    "#Function call\n",
    "df_t1_processed = processData(df_t1)\n",
    "df_t2_processed = processData(df_t2)\n",
    "df_t3_processed = processData(df_t3)\n",
    "\n",
    "print('Count:', df_t1_processed.shape[0])\n",
    "print('Count:', df_t2_processed.shape[0])\n",
    "print('Count:', df_t3_processed.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332e1ec",
   "metadata": {},
   "source": [
    "### Measuring Indicators after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61da29e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compliance:  0.9994957135653051\n",
      "Credability:  1.0\n",
      "Validity for Time frame 1:  0.9997478567826525\n",
      "Compliance:  0.9994107023659297\n",
      "Credability:  1.0\n",
      "Validity for Time frame 2:  0.9997053511829648\n",
      "Compliance:  0.9891048518055969\n",
      "Credability:  1.0\n",
      "Validity for Time frame 3:  0.9945524259027985\n"
     ]
    }
   ],
   "source": [
    "#Validity\n",
    "# Validity for Time frame 1\n",
    "print(\"Validity for Time frame 1: \", getValidity(getCompliance([df_t1_processed]), 0.5, getCredability([df_t1_processed]), 0.5))\n",
    "\n",
    "# Validity for Time frame 2\n",
    "print(\"Validity for Time frame 2: \", getValidity(getCompliance([df_t1_processed, df_t2_processed]), 0.5, getCredability([df_t1_processed, df_t2_processed]), 0.5))\n",
    "\n",
    "# Validity for Time frame 3\n",
    "print(\"Validity for Time frame 3: \", getValidity(getCompliance([df_t1_processed, df_t2_processed, df_t3_processed]), 0.5, getCredability([df_t1_processed, df_t2_processed, df_t3_processed]), 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35cbfe",
   "metadata": {},
   "source": [
    "### Visualizing and Interpreting Indicators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
